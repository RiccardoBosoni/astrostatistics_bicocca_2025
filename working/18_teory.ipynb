{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134b7179-2c6f-411e-8963-7595c6be9fe3",
   "metadata": {},
   "source": [
    "# Understanding Regularization in Polynomial Regression\n",
    "\n",
    "## The Overfitting Problem\n",
    "\n",
    "When you have a high-degree polynomial, the model can become extremely flexible and starts \"chasing\" individual data points, especially where data is sparse (like between those last two points). This creates:\n",
    "- **High variance** between data points\n",
    "- **Poor generalization** to new data\n",
    "- **Erratic behavior** in cross-validation\n",
    "\n",
    "## What is Regularization?\n",
    "\n",
    "Regularization adds a \"penalty term\" to discourage overly complex models. Instead of just minimizing the fit error, you minimize:\n",
    "\n",
    "**Original (unregularized):**\n",
    "$$(Y - M\\theta)^T C^{-1} (Y - M\\theta)$$\n",
    "This is just \"how well do I fit the data?\"\n",
    "\n",
    "**Regularized (Ridge Regression):**\n",
    "$$(Y - M \\theta)^T C^{-1} (Y- M \\theta) + \\lambda\\, \\theta^T\\theta$$\n",
    "This is \"how well do I fit the data + **how large are my coefficients?**\"\n",
    "\n",
    "## Understanding the Terms\n",
    "\n",
    "1. **First term**: $(Y - M\\theta)^T C^{-1} (Y - M\\theta)$ \n",
    "   - Measures fit quality (smaller = better fit)\n",
    "   - This is your weighted sum of squared residuals\n",
    "\n",
    "2. **Second term**: $\\lambda\\, \\theta^T\\theta$\n",
    "   - Penalty for large coefficients (smaller coefficients = simpler model)\n",
    "   - $\\theta^T\\theta = \\theta_1^2 + \\theta_2^2 + ... + \\theta_n^2$ (sum of squared coefficients)\n",
    "   - $\\lambda$ controls how much you care about this penalty\n",
    "\n",
    "## The Regularization Parameter Î»\n",
    "\n",
    "- **Î» = 0**: No penalty â†’ can overfit\n",
    "- **Î» small**: Gentle penalty â†’ allows some flexibility\n",
    "- **Î» large**: Strong penalty â†’ forces simpler model (may underfit)\n",
    "- **Î» â†’ âˆž**: All coefficients forced to ~0 â†’ extreme underfitting\n",
    "\n",
    "## The Solution\n",
    "\n",
    "The regularized solution becomes:\n",
    "$$\\hat\\theta = (M^T C^{-1} M + \\lambda I)^{-1} (M^T C^{-1} Y)$$\n",
    "\n",
    "Notice the **$+ \\lambda I$** term - this is the key difference! It:\n",
    "- Makes the matrix inversion more stable\n",
    "- Shrinks coefficients toward zero\n",
    "- Prevents any single coefficient from becoming too large\n",
    "\n",
    "## Bayesian Interpretation\n",
    "\n",
    "From a Bayesian view, adding $\\lambda\\, \\theta^T\\theta$ is equivalent to saying:\n",
    "\n",
    "**\"I believe a priori that coefficients should be small\"**\n",
    "\n",
    "The prior is:\n",
    "$$p(\\theta | I ) \\propto \\exp{\\left(\\frac{-\\lambda\\, \\theta^T\\theta}{2}\\right)}$$\n",
    "\n",
    "This is a **Gaussian prior centered at zero** - you're saying \"I expect coefficients near zero, and I'm increasingly surprised by larger values.\"\n",
    "\n",
    "## Why It Works\n",
    "\n",
    "High-degree polynomials need **large coefficients** to create those wild oscillations between data points. By penalizing large coefficients, regularization:\n",
    "\n",
    "1. **Smooths the function** - prevents wild swings\n",
    "2. **Reduces variance** - more stable predictions\n",
    "3. **Improves generalization** - better cross-validation performance\n",
    "\n",
    "## The Sweet Spot - Bias-Variance Tradeoff\n",
    "\n",
    "The goal is to find the balance between:\n",
    "- **Underfit** (Î» too large): High bias, low variance - model too simple\n",
    "- **Balanced** (Î» optimal): Good bias-variance tradeoff\n",
    "- **Overfit** (Î» = 0): Low bias, high variance - model too complex\n",
    "\n",
    "The optimal Î» minimizes test/validation error, giving you the best generalization to new data.\n",
    "\n",
    "---\n",
    "\n",
    "**In your cosmology context**: Even if you use a degree-5 polynomial, regularization can prevent those crazy oscillations between your last data points by keeping the high-order coefficients small!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30202f5f-c06d-45e8-a5dd-c4ddef3248e5",
   "metadata": {},
   "source": [
    "# Least Absolute Shrinkage and Selection (LASSO) Regularization\n",
    "\n",
    "## What is LASSO?\n",
    "\n",
    "An alternative to Ridge Regression is **LASSO**, which uses a different penalty term:\n",
    "\n",
    "**LASSO regularization:**\n",
    "$$(Y - M \\theta)^T(Y- M \\theta) + \\lambda \\sum_i|\\theta_i|$$\n",
    "\n",
    "This is equivalent to least-squares minimization with the restriction that:\n",
    "$$ \\sum_i|\\theta_i| < s$$\n",
    "\n",
    "## Key Difference from Ridge Regression\n",
    "\n",
    "**Ridge**: Penalty on the sum of **squared** coefficients â†’ $\\lambda\\, \\theta^T\\theta = \\lambda \\sum_i \\theta_i^2$\n",
    "\n",
    "**LASSO**: Penalty on the sum of **absolute values** â†’ $\\lambda \\sum_i|\\theta_i|$\n",
    "\n",
    "## The Power of L1 Regularization\n",
    "\n",
    "The absolute value penalty has a special property: it **preferentially selects regions of likelihood space that coincide with one of the vertices** within the region defined by the regularization.\n",
    "\n",
    "### What does this mean?\n",
    "\n",
    "***LASSO has the net effect of setting one (or more) of the model coefficients to exactly zero.***\n",
    "\n",
    "## LASSO vs Ridge: Feature Selection\n",
    "\n",
    "| Property | Ridge Regression | LASSO |\n",
    "|----------|------------------|-------|\n",
    "| Penalty | $\\sum_i \\theta_i^2$ | $\\sum_i \\|\\theta_i\\|$ |\n",
    "| Coefficient behavior | Shrinks toward zero | Sets some to **exactly zero** |\n",
    "| Feature selection | No (keeps all features) | Yes (automatic feature selection) |\n",
    "| Sparsity | No | Yes |\n",
    "\n",
    "## Why LASSO Creates Sparsity\n",
    "\n",
    "The L1 penalty (absolute value) creates \"corners\" in the constraint region. When the likelihood contours intersect these corners, coefficients hit exactly zero.\n",
    "\n",
    "**Ridge** (L2): Smooth circular/elliptical constraint â†’ coefficients get small but rarely exactly zero\n",
    "\n",
    "**LASSO** (L1): Diamond-shaped constraint with corners â†’ coefficients often hit exactly zero at the corners\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "1. **Automatic feature selection**: LASSO identifies which polynomial terms are truly necessary\n",
    "2. **Interpretability**: Simpler models with fewer non-zero coefficients\n",
    "3. **Sparsity**: Removes irrelevant features entirely\n",
    "\n",
    "## When to Use LASSO vs Ridge\n",
    "\n",
    "- **Use LASSO when**: \n",
    "  - You suspect only some features are important\n",
    "  - You want automatic feature selection\n",
    "  - You prefer a sparse, interpretable model\n",
    "\n",
    "- **Use Ridge when**:\n",
    "  - You believe most features contribute\n",
    "  - You want to shrink all coefficients smoothly\n",
    "  - Coefficients are highly correlated\n",
    "\n",
    "---\n",
    "\n",
    "**In your polynomial regression**: LASSO might decide that only degrees 1, 2, and 4 matter, setting the coefficients for degrees 3 and 5 to exactly zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9efc03-6eec-451a-b0e7-994b533f25fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "647db57f-d8c0-48a9-b4b7-05621a21a99d",
   "metadata": {},
   "source": [
    "# Gaussian Process Kernel Selection Guide\n",
    "\n",
    "## Understanding Kernel Choice\n",
    "\n",
    "Different kernels encode different assumptions about your function. For **cosmological distance modulus vs redshift**, here's what you should consider:\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Kernels for Your Data\n",
    "\n",
    "### 1. **MatÃ©rn Kernel** (BEST for cosmology â­)\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "```\n",
    "\n",
    "**Why it's good:**\n",
    "- âœ… Flexible smoothness control via `nu` parameter\n",
    "- âœ… Not infinitely smooth (more realistic for real data)\n",
    "- âœ… Works well with noisy measurements\n",
    "- âœ… **Standard choice in cosmology and astrophysics**\n",
    "\n",
    "**The `nu` parameter:**\n",
    "- `nu = 0.5`: Very rough (equivalent to Exponential kernel)\n",
    "- `nu = 1.5`: Once differentiable (**good default**)\n",
    "- `nu = 2.5`: Twice differentiable (**smooth, good for cosmology**)\n",
    "- `nu â†’ âˆž`: Approaches RBF (infinitely smooth)\n",
    "\n",
    "**For distance modulus:** Use `nu = 2.5` or `nu = 1.5`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **RBF (Radial Basis Function)** - Also called Squared Exponential\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = ConstantKernel(1.0) * RBF(length_scale=10.0)\n",
    "```\n",
    "\n",
    "**Why it's good:**\n",
    "- âœ… Produces very smooth functions\n",
    "- âœ… Good for continuous, smooth cosmological relations\n",
    "- âœ… Simple - only one hyperparameter (length_scale)\n",
    "\n",
    "**Why it might be too simple:**\n",
    "- âš ï¸ Infinitely differentiable (too smooth for real data?)\n",
    "- âš ï¸ Can't capture sharp transitions\n",
    "- âš ï¸ May oversmooth noisy regions\n",
    "\n",
    "**For distance modulus:** Good if you expect perfectly smooth cosmological curve\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Rational Quadratic** (Good alternative)\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "\n",
    "kernel = ConstantKernel(1.0) * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "```\n",
    "\n",
    "**Why it's good:**\n",
    "- âœ… Mixture of RBF kernels with different length scales\n",
    "- âœ… More flexible than RBF\n",
    "- âœ… Can capture multi-scale structure\n",
    "\n",
    "**The `alpha` parameter:**\n",
    "- Controls the relative weighting of large-scale vs small-scale variations\n",
    "- Larger `alpha` â†’ more like RBF (single scale)\n",
    "- Smaller `alpha` â†’ captures multiple scales\n",
    "\n",
    "**For distance modulus:** Good if data has features at different scales\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Exponential Sine Squared** (For periodic features)\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "\n",
    "kernel = ConstantKernel(1.0) * ExpSineSquared(length_scale=1.0, periodicity=1.0)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- âŒ **NOT for distance modulus** (not periodic!)\n",
    "- âœ… Good for: stellar light curves, climate data, seasonal patterns\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Composite Kernels** (Most flexible â­â­)\n",
    "\n",
    "You can **combine** kernels with `+` and `*` operators!\n",
    "\n",
    "#### Example 1: Smooth trend + small-scale noise\n",
    "```python\n",
    "# Long-term smooth trend + short-term variations\n",
    "kernel = (ConstantKernel(1.0) * Matern(length_scale=10.0, nu=2.5) +\n",
    "          ConstantKernel(0.1) * Matern(length_scale=0.5, nu=1.5))\n",
    "```\n",
    "\n",
    "#### Example 2: Multiplicative (for non-stationary data)\n",
    "```python\n",
    "# Amplitude varies with redshift\n",
    "kernel = (ConstantKernel(1.0) * RBF(length_scale=10.0) * \n",
    "          RBF(length_scale=1.0))\n",
    "```\n",
    "\n",
    "#### Example 3: Additive (for multiple components)\n",
    "```python\n",
    "# Main signal + noise\n",
    "kernel = (ConstantKernel(10.0) * Matern(length_scale=5.0, nu=2.5) +\n",
    "          WhiteKernel(noise_level=0.1))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## My Specific Recommendations for Distance Modulus\n",
    "\n",
    "### **Option 1: Simple and Robust** (Start here!)\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "kernel = ConstantKernel(1.0, constant_value_bounds=(0.1, 100)) * \\\n",
    "         Matern(length_scale=5.0, length_scale_bounds=(0.1, 50), nu=2.5)\n",
    "```\n",
    "\n",
    "**Why:** Smooth enough for cosmology, but not over-smooth. Standard choice.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: More Flexible** (If Option 1 underfits)\n",
    "```python\n",
    "kernel = ConstantKernel(1.0, constant_value_bounds=(0.1, 100)) * \\\n",
    "         Matern(length_scale=5.0, length_scale_bounds=(0.1, 50), nu=1.5)\n",
    "```\n",
    "\n",
    "**Why:** `nu=1.5` is less smooth than `nu=2.5`, can capture more detail.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 3: Multi-scale** (If you see structure at different scales)\n",
    "```python\n",
    "# Long-range smooth component + short-range details\n",
    "kernel = (ConstantKernel(5.0) * Matern(length_scale=10.0, nu=2.5) +\n",
    "          ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5))\n",
    "```\n",
    "\n",
    "**Why:** Captures both the overall cosmological trend AND local variations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 4: Let sklearn optimize** (Automated!)\n",
    "```python\n",
    "# Use optimizable bounds and let GP find best hyperparameters\n",
    "kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \\\n",
    "         Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=1.5)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu**2, \n",
    "                               n_restarts_optimizer=10)\n",
    "gp.fit(z_sample.reshape(-1, 1), mu_sample)\n",
    "\n",
    "print(f\"Optimized kernel: {gp.kernel_}\")\n",
    "```\n",
    "\n",
    "**Why:** GP automatically finds best hyperparameters via maximum likelihood. Set `n_restarts_optimizer=10` to avoid local minima.\n",
    "\n",
    "---\n",
    "\n",
    "## Kernel Comparison Table\n",
    "\n",
    "| Kernel | Smoothness | Flexibility | Cosmology? | Parameters |\n",
    "|--------|-----------|-------------|------------|------------|\n",
    "| **RBF** | âˆž smooth | Low | âœ… Good | `length_scale` |\n",
    "| **MatÃ©rn (nu=2.5)** | Smooth | Medium | â­ Best | `length_scale, nu` |\n",
    "| **MatÃ©rn (nu=1.5)** | Moderate | High | âœ… Good | `length_scale, nu` |\n",
    "| **Rational Quadratic** | Variable | High | âœ… Good | `length_scale, alpha` |\n",
    "| **Exponential** | Rough | Low | âŒ No | `length_scale` |\n",
    "| **Composite** | Custom | Very High | â­ Best | Multiple |\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Decision Tree\n",
    "\n",
    "```\n",
    "Do you know the smoothness level?\n",
    "â”œâ”€ Yes, very smooth â†’ Use RBF\n",
    "â””â”€ No / unsure\n",
    "   â”œâ”€ Single scale features â†’ Use MatÃ©rn (nu=2.5)\n",
    "   â””â”€ Multiple scale features â†’ Use composite kernel\n",
    "\n",
    "Is your model underfitting?\n",
    "â”œâ”€ Yes â†’ Decrease length_scale or use nu=1.5\n",
    "â””â”€ No â†’ Good!\n",
    "\n",
    "Is your model overfitting?\n",
    "â”œâ”€ Yes â†’ Increase length_scale or use nu=2.5\n",
    "â””â”€ No â†’ Good!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Code to Test Multiple Kernels\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import *\n",
    "\n",
    "kernels = {\n",
    "    'RBF': ConstantKernel(1.0) * RBF(length_scale=5.0),\n",
    "    'MatÃ©rn-1.5': ConstantKernel(1.0) * Matern(length_scale=5.0, nu=1.5),\n",
    "    'MatÃ©rn-2.5': ConstantKernel(1.0) * Matern(length_scale=5.0, nu=2.5),\n",
    "    'RatQuad': ConstantKernel(1.0) * RationalQuadratic(length_scale=5.0, alpha=1.0),\n",
    "    'Multi-scale': (ConstantKernel(5.0) * Matern(length_scale=10.0, nu=2.5) +\n",
    "                    ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5))\n",
    "}\n",
    "\n",
    "for name, kernel in kernels.items():\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu**2)\n",
    "    scores = cross_val_score(gp, z_sample.reshape(-1, 1), mu_sample, \n",
    "                            cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse = np.sqrt(-scores.mean())\n",
    "    print(f\"{name:15s}: RMSE = {rmse:.4f} Â± {np.sqrt(scores.std()):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR - Just tell me what to use!\n",
    "\n",
    "**For distance modulus vs redshift:**\n",
    "\n",
    "```python\n",
    "kernel = ConstantKernel(1.0) * Matern(length_scale=5.0, nu=2.5)\n",
    "```\n",
    "\n",
    "Then use GridSearchCV to optimize the hyperparameters. Done! ðŸŽ‰\n",
    "\n",
    "If that underfits, try `nu=1.5` for more flexibility.\n",
    "If it overfits, try RBF for maximum smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cef81f-4197-465c-aae5-10ccb3f7a093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1406131-2a83-4986-b7d7-9d0ba5179532",
   "metadata": {},
   "source": [
    "# Parametric vs Data-Driven Fits: A Complete Guide\n",
    "\n",
    "## Quick Definition\n",
    "\n",
    "| Aspect | Parametric | Data-Driven |\n",
    "|--------|-----------|-------------|\n",
    "| **Based on** | Physical theory/model | Data patterns only |\n",
    "| **Parameters** | Few (2-5), interpretable | Many, often not interpretable |\n",
    "| **Flexibility** | Limited by model | High - adapts to data |\n",
    "| **Extrapolation** | Good (theory-guided) | Poor (no theory) |\n",
    "| **Example** | Î›CDM cosmology | Gaussian Process |\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Explanation\n",
    "\n",
    "### Parametric Fit\n",
    "\n",
    "**Definition:** Fitting assumes a **specific functional form** derived from physical theory or mathematical model.\n",
    "\n",
    "**Key characteristics:**\n",
    "1. âœ… **Few parameters** - typically 2-10\n",
    "2. âœ… **Interpretable** - parameters have physical meaning\n",
    "3. âœ… **Theory-driven** - functional form from first principles\n",
    "4. âœ… **Extrapolates well** - physics guides behavior outside data range\n",
    "5. âŒ **Can be wrong** - if model assumptions violated\n",
    "6. âŒ **Inflexible** - can't capture unexpected features\n",
    "\n",
    "**Mathematical form:**\n",
    "$$y = f(x; \\theta_1, \\theta_2, ..., \\theta_n)$$\n",
    "\n",
    "Where:\n",
    "- f is a **known function** (from theory)\n",
    "- Î¸â‚, Î¸â‚‚, ... are the **parameters to fit**\n",
    "- n is **small** (usually < 10)\n",
    "\n",
    "---\n",
    "\n",
    "### Data-Driven Fit (Non-Parametric)\n",
    "\n",
    "**Definition:** Fitting makes **minimal assumptions** about functional form. Lets data determine the shape.\n",
    "\n",
    "**Key characteristics:**\n",
    "1. âœ… **Flexible** - adapts to any smooth pattern\n",
    "2. âœ… **No model assumptions** - doesn't require theory\n",
    "3. âœ… **Discovers unexpected features** - can reveal new physics\n",
    "4. âœ… **Model-independent** - unbiased by theory\n",
    "5. âŒ **Many hyperparameters** - kernel choice, length scales, etc.\n",
    "6. âŒ **Poor extrapolation** - no guidance outside data\n",
    "7. âŒ **Less interpretable** - hard to extract physical meaning\n",
    "\n",
    "**Mathematical form:**\n",
    "$$y = f(x) \\text{ where } f \\text{ is learned from data}$$\n",
    "\n",
    "Where:\n",
    "- f is **not specified in advance**\n",
    "- Shape determined by data + smoothness assumptions\n",
    "- Infinite-dimensional (not reducible to few parameters)\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Example: Supernova Distance Modulus\n",
    "\n",
    "### Your Data: Î¼(z) - Distance modulus vs redshift\n",
    "\n",
    "---\n",
    "\n",
    "### Parametric Approach: Î›CDM Model\n",
    "\n",
    "**Functional form from General Relativity:**\n",
    "$$\\mu(z) = 5 \\log_{10} \\left( \\frac{c}{H_0 \\cdot 10\\text{pc}} (1+z) \\int_0^z \\frac{dz'}{\\sqrt{\\Omega_m (1+z')^3 + \\Omega_\\Lambda}} \\right)$$\n",
    "\n",
    "**Parameters to fit:**\n",
    "- **Hâ‚€** = Hubble constant (expansion rate today)\n",
    "- **Î©â‚˜** = matter density parameter\n",
    "\n",
    "**Only 2 parameters!**\n",
    "\n",
    "**What you're doing:**\n",
    "- âœ… Using Einstein's equations\n",
    "- âœ… Assuming flat universe (Î©â‚˜ + Î©Î› = 1)\n",
    "- âœ… Assuming dark energy is cosmological constant\n",
    "- âœ… Getting physically meaningful results\n",
    "\n",
    "**Advantages:**\n",
    "```python\n",
    "# Simple, interpretable\n",
    "H0 = 70.0  # km/s/Mpc - has units, physical meaning\n",
    "Omega_m = 0.3  # dimensionless - fraction of universe that's matter\n",
    "```\n",
    "\n",
    "**Can answer:**\n",
    "- âœ“ What is the Hubble constant?\n",
    "- âœ“ How much matter vs dark energy?\n",
    "- âœ“ Will universe expand forever?\n",
    "- âœ“ What happens at z = 10 (extrapolation)?\n",
    "\n",
    "---\n",
    "\n",
    "### Data-Driven Approach: Gaussian Process\n",
    "\n",
    "**Functional form:**\n",
    "$$\\mu(z) \\sim \\mathcal{GP}(m(z), k(z, z'))$$\n",
    "\n",
    "No explicit equation! GP learns the function from data.\n",
    "\n",
    "**\"Parameters\" (actually hyperparameters):**\n",
    "- Kernel choice (RBF, MatÃ©rn, etc.)\n",
    "- Constant amplitude (ÏƒÂ²)\n",
    "- Length scale (â„“)\n",
    "- Smoothness parameter (Î½ for MatÃ©rn)\n",
    "\n",
    "**Many effective parameters** (infinite-dimensional function space)\n",
    "\n",
    "**What you're doing:**\n",
    "- âœ… Assuming smoothness (via kernel)\n",
    "- âœ… Letting data determine shape\n",
    "- âŒ NOT using physical theory\n",
    "- âŒ Parameters have no cosmological meaning\n",
    "\n",
    "**Advantages:**\n",
    "```python\n",
    "# Flexible, no theory needed\n",
    "length_scale = 5.3  # correlation length - no physical units\n",
    "constant = 12.7     # amplitude - just a scale factor\n",
    "# What do these mean physically? Nothing specific!\n",
    "```\n",
    "\n",
    "**Can answer:**\n",
    "- âœ“ What is Î¼ at z = 0.5 (interpolation)?\n",
    "- âœ“ How uncertain is the prediction?\n",
    "- âœ“ Are there unexpected features?\n",
    "- âœ— What is Hâ‚€? (Can't extract directly)\n",
    "- âœ— What happens at z = 10? (Unreliable extrapolation)\n",
    "\n",
    "---\n",
    "\n",
    "## Side-by-Side Comparison\n",
    "\n",
    "### Example: Polynomial Regression\n",
    "\n",
    "#### Parametric: Fixed degree polynomial\n",
    "```python\n",
    "# Assume quadratic relationship (theory suggests this)\n",
    "def model(x, a, b, c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "# Fit: find 3 parameters\n",
    "params, _ = curve_fit(model, x, y)\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- 3 parameters only\n",
    "- Assumes exactly quadratic (could be wrong!)\n",
    "- Extrapolates as parabola\n",
    "\n",
    "#### Data-Driven: Flexible polynomial degree\n",
    "```python\n",
    "# Try many polynomial degrees, pick best via cross-validation\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Let data choose complexity\n",
    "for degree in range(1, 15):\n",
    "    model = Ridge()\n",
    "    cv_score = cross_val_score(model, X_poly, y)\n",
    "    # Pick degree with best score\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- Many parameters (one per term)\n",
    "- No assumption about \"true\" degree\n",
    "- Cross-validation chooses complexity\n",
    "- May overfit or underfit\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Each\n",
    "\n",
    "### Use Parametric When:\n",
    "1. âœ… **You have a physical model** - theory predicts functional form\n",
    "2. âœ… **Interpretability matters** - need physically meaningful parameters\n",
    "3. âœ… **Extrapolation needed** - must predict outside data range\n",
    "4. âœ… **Small dataset** - few data points, need constraints\n",
    "5. âœ… **Publication/communication** - easier to explain and justify\n",
    "\n",
    "**Examples:**\n",
    "- Cosmological distance-redshift relation (Î›CDM)\n",
    "- Stellar evolution models\n",
    "- Orbital mechanics (Kepler's laws)\n",
    "- Exponential decay (radioactivity)\n",
    "- Power laws (scaling relations)\n",
    "\n",
    "### Use Data-Driven When:\n",
    "1. âœ… **No theory available** - exploring unknown territory\n",
    "2. âœ… **Theory might be wrong** - want unbiased look at data\n",
    "3. âœ… **Complex/unknown functional form** - too complicated to model\n",
    "4. âœ… **Interpolation only** - don't need to extrapolate\n",
    "5. âœ… **Large dataset** - enough data to constrain flexible model\n",
    "6. âœ… **Exploratory analysis** - discovering new patterns\n",
    "\n",
    "**Examples:**\n",
    "- Machine learning predictions\n",
    "- Detrending/smoothing noisy data\n",
    "- Anomaly detection\n",
    "- Image recognition\n",
    "- Speech recognition\n",
    "\n",
    "---\n",
    "\n",
    "## Hybrid Approaches\n",
    "\n",
    "Sometimes you combine both!\n",
    "\n",
    "### Example 1: Parametric + Gaussian Process Residuals\n",
    "```python\n",
    "# Parametric: main signal\n",
    "mu_theory = LCDM_model(z, H0, Omega_m)\n",
    "\n",
    "# Data-driven: capture deviations\n",
    "residuals = mu_obs - mu_theory\n",
    "gp = GaussianProcessRegressor()\n",
    "gp.fit(z, residuals)\n",
    "\n",
    "# Final prediction\n",
    "mu_final = mu_theory + gp.predict(z_new)\n",
    "```\n",
    "\n",
    "**Use case:** Î›CDM is mostly right, but there are small deviations (maybe new physics!)\n",
    "\n",
    "### Example 2: Physics-Informed Neural Networks\n",
    "```python\n",
    "# Neural network (data-driven) \n",
    "# but loss function includes physics equations (parametric constraints)\n",
    "loss = data_loss + lambda * physics_loss\n",
    "```\n",
    "\n",
    "**Use case:** Complex system where exact solution unknown, but physics provides constraints\n",
    "\n",
    "---\n",
    "\n",
    "## Philosophy: Two Ways of Knowing\n",
    "\n",
    "### Parametric (Deductive)\n",
    "**\"Top-down\" reasoning:**\n",
    "1. Start with theory (General Relativity)\n",
    "2. Derive predictions (Î¼(z) formula)\n",
    "3. Fit parameters to data (Hâ‚€, Î©â‚˜)\n",
    "4. Confirm or reject theory\n",
    "\n",
    "**Example reasoning:**\n",
    "> \"Einstein's equations predict this relationship. If the universe contains matter and dark energy in these proportions, we should see this distance-redshift curve.\"\n",
    "\n",
    "### Data-Driven (Inductive)\n",
    "**\"Bottom-up\" reasoning:**\n",
    "1. Start with data\n",
    "2. Find patterns\n",
    "3. Build empirical model\n",
    "4. (Maybe) develop theory later\n",
    "\n",
    "**Example reasoning:**\n",
    "> \"The data shows this smooth curve. I don't know why, but I can predict intermediate values accurately.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Tips\n",
    "\n",
    "### For Your Homework/Analysis:\n",
    "\n",
    "**First:** Do data-driven (GP) analysis\n",
    "- âœ… Explore the data without bias\n",
    "- âœ… Check for anomalies or unexpected features\n",
    "- âœ… Establish that data is smooth and well-behaved\n",
    "\n",
    "**Then:** Do parametric (Î›CDM) fit\n",
    "- âœ… Test physical model\n",
    "- âœ… Extract meaningful parameters (Hâ‚€, Î©â‚˜)\n",
    "- âœ… Compare to theory predictions\n",
    "\n",
    "**Finally:** Compare approaches\n",
    "- Do residuals (data - Î›CDM) show patterns?\n",
    "- Does GP capture something Î›CDM misses?\n",
    "- Is Î›CDM adequate or do we need new physics?\n",
    "\n",
    "---\n",
    "\n",
    "## Common Misconceptions\n",
    "\n",
    "### âŒ \"Parametric means linear regression\"\n",
    "**Reality:** Parametric just means fixed functional form. Can be highly nonlinear (like Î›CDM integral!)\n",
    "\n",
    "### âŒ \"Data-driven is always better because it's more flexible\"\n",
    "**Reality:** Flexibility can lead to overfitting. Without theory, you might fit noise.\n",
    "\n",
    "### âŒ \"GP has no parameters\"\n",
    "**Reality:** GP has hyperparameters (kernel, length scales). They're just not the \"parameters of interest\" like Hâ‚€.\n",
    "\n",
    "### âŒ \"Parametric models can't be wrong\"\n",
    "**Reality:** Theory-based models can absolutely be wrong! That's why we test them.\n",
    "\n",
    "### âŒ \"You must choose one approach\"\n",
    "**Reality:** Use both! They provide complementary information.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Analogy\n",
    "\n",
    "### Parametric: Following a Recipe\n",
    "- Recipe says: \"Bake at 180Â°C for 30 minutes\"\n",
    "- Based on theory: chemistry of baking\n",
    "- Parameters: temperature, time\n",
    "- If you follow recipe at 200Â°C for 60 min, you can predict (burnt cake)\n",
    "\n",
    "### Data-Driven: Tasting and Adjusting\n",
    "- Try different amounts, taste, adjust\n",
    "- No recipe, just experience\n",
    "- Learn empirically: \"More sugar â†’ sweeter\"\n",
    "- Can't predict what happens with caviar (outside training data)\n",
    "\n",
    "**Best approach:** Use recipe (theory) as starting point, but adjust based on tasting (data)!\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Feature | Parametric | Data-Driven |\n",
    "|---------|-----------|-------------|\n",
    "| **Basis** | Physical theory | Data patterns |\n",
    "| **# Parameters** | Few (2-10) | Many (or infinite) |\n",
    "| **Interpretability** | High | Low |\n",
    "| **Flexibility** | Low | High |\n",
    "| **Extrapolation** | Good | Poor |\n",
    "| **Overfitting risk** | Low | High |\n",
    "| **Requires theory** | Yes | No |\n",
    "| **Best for** | Well-understood physics | Exploratory analysis |\n",
    "| **Your SN example** | Î›CDM (Hâ‚€, Î©â‚˜) | Gaussian Process |\n",
    "\n",
    "---\n",
    "\n",
    "## The Bottom Line\n",
    "\n",
    "**Parametric:** \n",
    "> \"I know the physics, I just need to measure the parameters.\"\n",
    "\n",
    "**Data-Driven:** \n",
    "> \"I don't know the physics, let the data speak.\"\n",
    "\n",
    "**Best Practice:**\n",
    "> \"Use data-driven to explore, parametric to explain.\"\n",
    "\n",
    "For cosmology: Start with GP to ensure no surprises, then fit Î›CDM to extract physical parameters!\n",
    "\n",
    "ðŸŽ¯ **Remember:** Neither is \"better\" - they answer different questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b60f3-bcc9-4c8f-991b-7d5d6e8aaa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa41092-6f3f-488e-ab19-9fa445d8d7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0478f4-e1c3-4570-8bac-876ae04b2f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
