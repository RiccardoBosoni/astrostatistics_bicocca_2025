{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134b7179-2c6f-411e-8963-7595c6be9fe3",
   "metadata": {},
   "source": [
    "# Understanding Regularization in Polynomial Regression\n",
    "\n",
    "## The Overfitting Problem\n",
    "\n",
    "When you have a high-degree polynomial, the model can become extremely flexible and starts \"chasing\" individual data points, especially where data is sparse (like between those last two points). This creates:\n",
    "- **High variance** between data points\n",
    "- **Poor generalization** to new data\n",
    "- **Erratic behavior** in cross-validation\n",
    "\n",
    "## What is Regularization?\n",
    "\n",
    "Regularization adds a \"penalty term\" to discourage overly complex models. Instead of just minimizing the fit error, you minimize:\n",
    "\n",
    "**Original (unregularized):**\n",
    "$$(Y - M\\theta)^T C^{-1} (Y - M\\theta)$$\n",
    "This is just \"how well do I fit the data?\"\n",
    "\n",
    "**Regularized (Ridge Regression):**\n",
    "$$(Y - M \\theta)^T C^{-1} (Y- M \\theta) + \\lambda\\, \\theta^T\\theta$$\n",
    "This is \"how well do I fit the data + **how large are my coefficients?**\"\n",
    "\n",
    "## Understanding the Terms\n",
    "\n",
    "1. **First term**: $(Y - M\\theta)^T C^{-1} (Y - M\\theta)$ \n",
    "   - Measures fit quality (smaller = better fit)\n",
    "   - This is your weighted sum of squared residuals\n",
    "\n",
    "2. **Second term**: $\\lambda\\, \\theta^T\\theta$\n",
    "   - Penalty for large coefficients (smaller coefficients = simpler model)\n",
    "   - $\\theta^T\\theta = \\theta_1^2 + \\theta_2^2 + ... + \\theta_n^2$ (sum of squared coefficients)\n",
    "   - $\\lambda$ controls how much you care about this penalty\n",
    "\n",
    "## The Regularization Parameter λ\n",
    "\n",
    "- **λ = 0**: No penalty → can overfit\n",
    "- **λ small**: Gentle penalty → allows some flexibility\n",
    "- **λ large**: Strong penalty → forces simpler model (may underfit)\n",
    "- **λ → ∞**: All coefficients forced to ~0 → extreme underfitting\n",
    "\n",
    "## The Solution\n",
    "\n",
    "The regularized solution becomes:\n",
    "$$\\hat\\theta = (M^T C^{-1} M + \\lambda I)^{-1} (M^T C^{-1} Y)$$\n",
    "\n",
    "Notice the **$+ \\lambda I$** term - this is the key difference! It:\n",
    "- Makes the matrix inversion more stable\n",
    "- Shrinks coefficients toward zero\n",
    "- Prevents any single coefficient from becoming too large\n",
    "\n",
    "## Bayesian Interpretation\n",
    "\n",
    "From a Bayesian view, adding $\\lambda\\, \\theta^T\\theta$ is equivalent to saying:\n",
    "\n",
    "**\"I believe a priori that coefficients should be small\"**\n",
    "\n",
    "The prior is:\n",
    "$$p(\\theta | I ) \\propto \\exp{\\left(\\frac{-\\lambda\\, \\theta^T\\theta}{2}\\right)}$$\n",
    "\n",
    "This is a **Gaussian prior centered at zero** - you're saying \"I expect coefficients near zero, and I'm increasingly surprised by larger values.\"\n",
    "\n",
    "## Why It Works\n",
    "\n",
    "High-degree polynomials need **large coefficients** to create those wild oscillations between data points. By penalizing large coefficients, regularization:\n",
    "\n",
    "1. **Smooths the function** - prevents wild swings\n",
    "2. **Reduces variance** - more stable predictions\n",
    "3. **Improves generalization** - better cross-validation performance\n",
    "\n",
    "## The Sweet Spot - Bias-Variance Tradeoff\n",
    "\n",
    "The goal is to find the balance between:\n",
    "- **Underfit** (λ too large): High bias, low variance - model too simple\n",
    "- **Balanced** (λ optimal): Good bias-variance tradeoff\n",
    "- **Overfit** (λ = 0): Low bias, high variance - model too complex\n",
    "\n",
    "The optimal λ minimizes test/validation error, giving you the best generalization to new data.\n",
    "\n",
    "---\n",
    "\n",
    "**In your cosmology context**: Even if you use a degree-5 polynomial, regularization can prevent those crazy oscillations between your last data points by keeping the high-order coefficients small!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30202f5f-c06d-45e8-a5dd-c4ddef3248e5",
   "metadata": {},
   "source": [
    "# Least Absolute Shrinkage and Selection (LASSO) Regularization\n",
    "\n",
    "## What is LASSO?\n",
    "\n",
    "An alternative to Ridge Regression is **LASSO**, which uses a different penalty term:\n",
    "\n",
    "**LASSO regularization:**\n",
    "$$(Y - M \\theta)^T(Y- M \\theta) + \\lambda \\sum_i|\\theta_i|$$\n",
    "\n",
    "This is equivalent to least-squares minimization with the restriction that:\n",
    "$$ \\sum_i|\\theta_i| < s$$\n",
    "\n",
    "## Key Difference from Ridge Regression\n",
    "\n",
    "**Ridge**: Penalty on the sum of **squared** coefficients → $\\lambda\\, \\theta^T\\theta = \\lambda \\sum_i \\theta_i^2$\n",
    "\n",
    "**LASSO**: Penalty on the sum of **absolute values** → $\\lambda \\sum_i|\\theta_i|$\n",
    "\n",
    "## The Power of L1 Regularization\n",
    "\n",
    "The absolute value penalty has a special property: it **preferentially selects regions of likelihood space that coincide with one of the vertices** within the region defined by the regularization.\n",
    "\n",
    "### What does this mean?\n",
    "\n",
    "***LASSO has the net effect of setting one (or more) of the model coefficients to exactly zero.***\n",
    "\n",
    "## LASSO vs Ridge: Feature Selection\n",
    "\n",
    "| Property | Ridge Regression | LASSO |\n",
    "|----------|------------------|-------|\n",
    "| Penalty | $\\sum_i \\theta_i^2$ | $\\sum_i \\|\\theta_i\\|$ |\n",
    "| Coefficient behavior | Shrinks toward zero | Sets some to **exactly zero** |\n",
    "| Feature selection | No (keeps all features) | Yes (automatic feature selection) |\n",
    "| Sparsity | No | Yes |\n",
    "\n",
    "## Why LASSO Creates Sparsity\n",
    "\n",
    "The L1 penalty (absolute value) creates \"corners\" in the constraint region. When the likelihood contours intersect these corners, coefficients hit exactly zero.\n",
    "\n",
    "**Ridge** (L2): Smooth circular/elliptical constraint → coefficients get small but rarely exactly zero\n",
    "\n",
    "**LASSO** (L1): Diamond-shaped constraint with corners → coefficients often hit exactly zero at the corners\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "1. **Automatic feature selection**: LASSO identifies which polynomial terms are truly necessary\n",
    "2. **Interpretability**: Simpler models with fewer non-zero coefficients\n",
    "3. **Sparsity**: Removes irrelevant features entirely\n",
    "\n",
    "## When to Use LASSO vs Ridge\n",
    "\n",
    "- **Use LASSO when**: \n",
    "  - You suspect only some features are important\n",
    "  - You want automatic feature selection\n",
    "  - You prefer a sparse, interpretable model\n",
    "\n",
    "- **Use Ridge when**:\n",
    "  - You believe most features contribute\n",
    "  - You want to shrink all coefficients smoothly\n",
    "  - Coefficients are highly correlated\n",
    "\n",
    "---\n",
    "\n",
    "**In your polynomial regression**: LASSO might decide that only degrees 1, 2, and 4 matter, setting the coefficients for degrees 3 and 5 to exactly zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9efc03-6eec-451a-b0e7-994b533f25fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "647db57f-d8c0-48a9-b4b7-05621a21a99d",
   "metadata": {},
   "source": [
    "# Gaussian Process Kernel Selection Guide\n",
    "\n",
    "## Understanding Kernel Choice\n",
    "\n",
    "Different kernels encode different assumptions about your function. For **cosmological distance modulus vs redshift**, here's what you should consider:\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Kernels for Your Data\n",
    "\n",
    "### 1. **Matérn Kernel** (BEST for cosmology ⭐)\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "```\n",
    "\n",
    "**Why it's good:**\n",
    "- ✅ Flexible smoothness control via `nu` parameter\n",
    "- ✅ Not infinitely smooth (more realistic for real data)\n",
    "- ✅ Works well with noisy measurements\n",
    "- ✅ **Standard choice in cosmology and astrophysics**\n",
    "\n",
    "**The `nu` parameter:**\n",
    "- `nu = 0.5`: Very rough (equivalent to Exponential kernel)\n",
    "- `nu = 1.5`: Once differentiable (**good default**)\n",
    "- `nu = 2.5`: Twice differentiable (**smooth, good for cosmology**)\n",
    "- `nu → ∞`: Approaches RBF (infinitely smooth)\n",
    "\n",
    "**For distance modulus:** Use `nu = 2.5` or `nu = 1.5`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **RBF (Radial Basis Function)** - Also called Squared Exponential\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = ConstantKernel(1.0) * RBF(length_scale=10.0)\n",
    "```\n",
    "\n",
    "**Why it's good:**\n",
    "- ✅ Produces very smooth functions\n",
    "- ✅ Good for continuous, smooth cosmological relations\n",
    "- ✅ Simple - only one hyperparameter (length_scale)\n",
    "\n",
    "**Why it might be too simple:**\n",
    "- ⚠️ Infinitely differentiable (too smooth for real data?)\n",
    "- ⚠️ Can't capture sharp transitions\n",
    "- ⚠️ May oversmooth noisy regions\n",
    "\n",
    "**For distance modulus:** Good if you expect perfectly smooth cosmological curve\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Rational Quadratic** (Good alternative)\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "\n",
    "kernel = ConstantKernel(1.0) * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "```\n",
    "\n",
    "**Why it's good:**\n",
    "- ✅ Mixture of RBF kernels with different length scales\n",
    "- ✅ More flexible than RBF\n",
    "- ✅ Can capture multi-scale structure\n",
    "\n",
    "**The `alpha` parameter:**\n",
    "- Controls the relative weighting of large-scale vs small-scale variations\n",
    "- Larger `alpha` → more like RBF (single scale)\n",
    "- Smaller `alpha` → captures multiple scales\n",
    "\n",
    "**For distance modulus:** Good if data has features at different scales\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Exponential Sine Squared** (For periodic features)\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "\n",
    "kernel = ConstantKernel(1.0) * ExpSineSquared(length_scale=1.0, periodicity=1.0)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- ❌ **NOT for distance modulus** (not periodic!)\n",
    "- ✅ Good for: stellar light curves, climate data, seasonal patterns\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Composite Kernels** (Most flexible ⭐⭐)\n",
    "\n",
    "You can **combine** kernels with `+` and `*` operators!\n",
    "\n",
    "#### Example 1: Smooth trend + small-scale noise\n",
    "```python\n",
    "# Long-term smooth trend + short-term variations\n",
    "kernel = (ConstantKernel(1.0) * Matern(length_scale=10.0, nu=2.5) +\n",
    "          ConstantKernel(0.1) * Matern(length_scale=0.5, nu=1.5))\n",
    "```\n",
    "\n",
    "#### Example 2: Multiplicative (for non-stationary data)\n",
    "```python\n",
    "# Amplitude varies with redshift\n",
    "kernel = (ConstantKernel(1.0) * RBF(length_scale=10.0) * \n",
    "          RBF(length_scale=1.0))\n",
    "```\n",
    "\n",
    "#### Example 3: Additive (for multiple components)\n",
    "```python\n",
    "# Main signal + noise\n",
    "kernel = (ConstantKernel(10.0) * Matern(length_scale=5.0, nu=2.5) +\n",
    "          WhiteKernel(noise_level=0.1))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## My Specific Recommendations for Distance Modulus\n",
    "\n",
    "### **Option 1: Simple and Robust** (Start here!)\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "kernel = ConstantKernel(1.0, constant_value_bounds=(0.1, 100)) * \\\n",
    "         Matern(length_scale=5.0, length_scale_bounds=(0.1, 50), nu=2.5)\n",
    "```\n",
    "\n",
    "**Why:** Smooth enough for cosmology, but not over-smooth. Standard choice.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: More Flexible** (If Option 1 underfits)\n",
    "```python\n",
    "kernel = ConstantKernel(1.0, constant_value_bounds=(0.1, 100)) * \\\n",
    "         Matern(length_scale=5.0, length_scale_bounds=(0.1, 50), nu=1.5)\n",
    "```\n",
    "\n",
    "**Why:** `nu=1.5` is less smooth than `nu=2.5`, can capture more detail.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 3: Multi-scale** (If you see structure at different scales)\n",
    "```python\n",
    "# Long-range smooth component + short-range details\n",
    "kernel = (ConstantKernel(5.0) * Matern(length_scale=10.0, nu=2.5) +\n",
    "          ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5))\n",
    "```\n",
    "\n",
    "**Why:** Captures both the overall cosmological trend AND local variations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 4: Let sklearn optimize** (Automated!)\n",
    "```python\n",
    "# Use optimizable bounds and let GP find best hyperparameters\n",
    "kernel = ConstantKernel(1.0, constant_value_bounds=(1e-3, 1e3)) * \\\n",
    "         Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2), nu=1.5)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu**2, \n",
    "                               n_restarts_optimizer=10)\n",
    "gp.fit(z_sample.reshape(-1, 1), mu_sample)\n",
    "\n",
    "print(f\"Optimized kernel: {gp.kernel_}\")\n",
    "```\n",
    "\n",
    "**Why:** GP automatically finds best hyperparameters via maximum likelihood. Set `n_restarts_optimizer=10` to avoid local minima.\n",
    "\n",
    "---\n",
    "\n",
    "## Kernel Comparison Table\n",
    "\n",
    "| Kernel | Smoothness | Flexibility | Cosmology? | Parameters |\n",
    "|--------|-----------|-------------|------------|------------|\n",
    "| **RBF** | ∞ smooth | Low | ✅ Good | `length_scale` |\n",
    "| **Matérn (nu=2.5)** | Smooth | Medium | ⭐ Best | `length_scale, nu` |\n",
    "| **Matérn (nu=1.5)** | Moderate | High | ✅ Good | `length_scale, nu` |\n",
    "| **Rational Quadratic** | Variable | High | ✅ Good | `length_scale, alpha` |\n",
    "| **Exponential** | Rough | Low | ❌ No | `length_scale` |\n",
    "| **Composite** | Custom | Very High | ⭐ Best | Multiple |\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Decision Tree\n",
    "\n",
    "```\n",
    "Do you know the smoothness level?\n",
    "├─ Yes, very smooth → Use RBF\n",
    "└─ No / unsure\n",
    "   ├─ Single scale features → Use Matérn (nu=2.5)\n",
    "   └─ Multiple scale features → Use composite kernel\n",
    "\n",
    "Is your model underfitting?\n",
    "├─ Yes → Decrease length_scale or use nu=1.5\n",
    "└─ No → Good!\n",
    "\n",
    "Is your model overfitting?\n",
    "├─ Yes → Increase length_scale or use nu=2.5\n",
    "└─ No → Good!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Code to Test Multiple Kernels\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process.kernels import *\n",
    "\n",
    "kernels = {\n",
    "    'RBF': ConstantKernel(1.0) * RBF(length_scale=5.0),\n",
    "    'Matérn-1.5': ConstantKernel(1.0) * Matern(length_scale=5.0, nu=1.5),\n",
    "    'Matérn-2.5': ConstantKernel(1.0) * Matern(length_scale=5.0, nu=2.5),\n",
    "    'RatQuad': ConstantKernel(1.0) * RationalQuadratic(length_scale=5.0, alpha=1.0),\n",
    "    'Multi-scale': (ConstantKernel(5.0) * Matern(length_scale=10.0, nu=2.5) +\n",
    "                    ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5))\n",
    "}\n",
    "\n",
    "for name, kernel in kernels.items():\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu**2)\n",
    "    scores = cross_val_score(gp, z_sample.reshape(-1, 1), mu_sample, \n",
    "                            cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse = np.sqrt(-scores.mean())\n",
    "    print(f\"{name:15s}: RMSE = {rmse:.4f} ± {np.sqrt(scores.std()):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR - Just tell me what to use!\n",
    "\n",
    "**For distance modulus vs redshift:**\n",
    "\n",
    "```python\n",
    "kernel = ConstantKernel(1.0) * Matern(length_scale=5.0, nu=2.5)\n",
    "```\n",
    "\n",
    "Then use GridSearchCV to optimize the hyperparameters. Done! 🎉\n",
    "\n",
    "If that underfits, try `nu=1.5` for more flexibility.\n",
    "If it overfits, try RBF for maximum smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cef81f-4197-465c-aae5-10ccb3f7a093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1406131-2a83-4986-b7d7-9d0ba5179532",
   "metadata": {},
   "source": [
    "# Parametric vs Data-Driven Fits: A Complete Guide\n",
    "\n",
    "## Quick Definition\n",
    "\n",
    "| Aspect | Parametric | Data-Driven |\n",
    "|--------|-----------|-------------|\n",
    "| **Based on** | Physical theory/model | Data patterns only |\n",
    "| **Parameters** | Few (2-5), interpretable | Many, often not interpretable |\n",
    "| **Flexibility** | Limited by model | High - adapts to data |\n",
    "| **Extrapolation** | Good (theory-guided) | Poor (no theory) |\n",
    "| **Example** | ΛCDM cosmology | Gaussian Process |\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Explanation\n",
    "\n",
    "### Parametric Fit\n",
    "\n",
    "**Definition:** Fitting assumes a **specific functional form** derived from physical theory or mathematical model.\n",
    "\n",
    "**Key characteristics:**\n",
    "1. ✅ **Few parameters** - typically 2-10\n",
    "2. ✅ **Interpretable** - parameters have physical meaning\n",
    "3. ✅ **Theory-driven** - functional form from first principles\n",
    "4. ✅ **Extrapolates well** - physics guides behavior outside data range\n",
    "5. ❌ **Can be wrong** - if model assumptions violated\n",
    "6. ❌ **Inflexible** - can't capture unexpected features\n",
    "\n",
    "**Mathematical form:**\n",
    "$$y = f(x; \\theta_1, \\theta_2, ..., \\theta_n)$$\n",
    "\n",
    "Where:\n",
    "- f is a **known function** (from theory)\n",
    "- θ₁, θ₂, ... are the **parameters to fit**\n",
    "- n is **small** (usually < 10)\n",
    "\n",
    "---\n",
    "\n",
    "### Data-Driven Fit (Non-Parametric)\n",
    "\n",
    "**Definition:** Fitting makes **minimal assumptions** about functional form. Lets data determine the shape.\n",
    "\n",
    "**Key characteristics:**\n",
    "1. ✅ **Flexible** - adapts to any smooth pattern\n",
    "2. ✅ **No model assumptions** - doesn't require theory\n",
    "3. ✅ **Discovers unexpected features** - can reveal new physics\n",
    "4. ✅ **Model-independent** - unbiased by theory\n",
    "5. ❌ **Many hyperparameters** - kernel choice, length scales, etc.\n",
    "6. ❌ **Poor extrapolation** - no guidance outside data\n",
    "7. ❌ **Less interpretable** - hard to extract physical meaning\n",
    "\n",
    "**Mathematical form:**\n",
    "$$y = f(x) \\text{ where } f \\text{ is learned from data}$$\n",
    "\n",
    "Where:\n",
    "- f is **not specified in advance**\n",
    "- Shape determined by data + smoothness assumptions\n",
    "- Infinite-dimensional (not reducible to few parameters)\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Example: Supernova Distance Modulus\n",
    "\n",
    "### Your Data: μ(z) - Distance modulus vs redshift\n",
    "\n",
    "---\n",
    "\n",
    "### Parametric Approach: ΛCDM Model\n",
    "\n",
    "**Functional form from General Relativity:**\n",
    "$$\\mu(z) = 5 \\log_{10} \\left( \\frac{c}{H_0 \\cdot 10\\text{pc}} (1+z) \\int_0^z \\frac{dz'}{\\sqrt{\\Omega_m (1+z')^3 + \\Omega_\\Lambda}} \\right)$$\n",
    "\n",
    "**Parameters to fit:**\n",
    "- **H₀** = Hubble constant (expansion rate today)\n",
    "- **Ωₘ** = matter density parameter\n",
    "\n",
    "**Only 2 parameters!**\n",
    "\n",
    "**What you're doing:**\n",
    "- ✅ Using Einstein's equations\n",
    "- ✅ Assuming flat universe (Ωₘ + ΩΛ = 1)\n",
    "- ✅ Assuming dark energy is cosmological constant\n",
    "- ✅ Getting physically meaningful results\n",
    "\n",
    "**Advantages:**\n",
    "```python\n",
    "# Simple, interpretable\n",
    "H0 = 70.0  # km/s/Mpc - has units, physical meaning\n",
    "Omega_m = 0.3  # dimensionless - fraction of universe that's matter\n",
    "```\n",
    "\n",
    "**Can answer:**\n",
    "- ✓ What is the Hubble constant?\n",
    "- ✓ How much matter vs dark energy?\n",
    "- ✓ Will universe expand forever?\n",
    "- ✓ What happens at z = 10 (extrapolation)?\n",
    "\n",
    "---\n",
    "\n",
    "### Data-Driven Approach: Gaussian Process\n",
    "\n",
    "**Functional form:**\n",
    "$$\\mu(z) \\sim \\mathcal{GP}(m(z), k(z, z'))$$\n",
    "\n",
    "No explicit equation! GP learns the function from data.\n",
    "\n",
    "**\"Parameters\" (actually hyperparameters):**\n",
    "- Kernel choice (RBF, Matérn, etc.)\n",
    "- Constant amplitude (σ²)\n",
    "- Length scale (ℓ)\n",
    "- Smoothness parameter (ν for Matérn)\n",
    "\n",
    "**Many effective parameters** (infinite-dimensional function space)\n",
    "\n",
    "**What you're doing:**\n",
    "- ✅ Assuming smoothness (via kernel)\n",
    "- ✅ Letting data determine shape\n",
    "- ❌ NOT using physical theory\n",
    "- ❌ Parameters have no cosmological meaning\n",
    "\n",
    "**Advantages:**\n",
    "```python\n",
    "# Flexible, no theory needed\n",
    "length_scale = 5.3  # correlation length - no physical units\n",
    "constant = 12.7     # amplitude - just a scale factor\n",
    "# What do these mean physically? Nothing specific!\n",
    "```\n",
    "\n",
    "**Can answer:**\n",
    "- ✓ What is μ at z = 0.5 (interpolation)?\n",
    "- ✓ How uncertain is the prediction?\n",
    "- ✓ Are there unexpected features?\n",
    "- ✗ What is H₀? (Can't extract directly)\n",
    "- ✗ What happens at z = 10? (Unreliable extrapolation)\n",
    "\n",
    "---\n",
    "\n",
    "## Side-by-Side Comparison\n",
    "\n",
    "### Example: Polynomial Regression\n",
    "\n",
    "#### Parametric: Fixed degree polynomial\n",
    "```python\n",
    "# Assume quadratic relationship (theory suggests this)\n",
    "def model(x, a, b, c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "# Fit: find 3 parameters\n",
    "params, _ = curve_fit(model, x, y)\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- 3 parameters only\n",
    "- Assumes exactly quadratic (could be wrong!)\n",
    "- Extrapolates as parabola\n",
    "\n",
    "#### Data-Driven: Flexible polynomial degree\n",
    "```python\n",
    "# Try many polynomial degrees, pick best via cross-validation\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Let data choose complexity\n",
    "for degree in range(1, 15):\n",
    "    model = Ridge()\n",
    "    cv_score = cross_val_score(model, X_poly, y)\n",
    "    # Pick degree with best score\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- Many parameters (one per term)\n",
    "- No assumption about \"true\" degree\n",
    "- Cross-validation chooses complexity\n",
    "- May overfit or underfit\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use Each\n",
    "\n",
    "### Use Parametric When:\n",
    "1. ✅ **You have a physical model** - theory predicts functional form\n",
    "2. ✅ **Interpretability matters** - need physically meaningful parameters\n",
    "3. ✅ **Extrapolation needed** - must predict outside data range\n",
    "4. ✅ **Small dataset** - few data points, need constraints\n",
    "5. ✅ **Publication/communication** - easier to explain and justify\n",
    "\n",
    "**Examples:**\n",
    "- Cosmological distance-redshift relation (ΛCDM)\n",
    "- Stellar evolution models\n",
    "- Orbital mechanics (Kepler's laws)\n",
    "- Exponential decay (radioactivity)\n",
    "- Power laws (scaling relations)\n",
    "\n",
    "### Use Data-Driven When:\n",
    "1. ✅ **No theory available** - exploring unknown territory\n",
    "2. ✅ **Theory might be wrong** - want unbiased look at data\n",
    "3. ✅ **Complex/unknown functional form** - too complicated to model\n",
    "4. ✅ **Interpolation only** - don't need to extrapolate\n",
    "5. ✅ **Large dataset** - enough data to constrain flexible model\n",
    "6. ✅ **Exploratory analysis** - discovering new patterns\n",
    "\n",
    "**Examples:**\n",
    "- Machine learning predictions\n",
    "- Detrending/smoothing noisy data\n",
    "- Anomaly detection\n",
    "- Image recognition\n",
    "- Speech recognition\n",
    "\n",
    "---\n",
    "\n",
    "## Hybrid Approaches\n",
    "\n",
    "Sometimes you combine both!\n",
    "\n",
    "### Example 1: Parametric + Gaussian Process Residuals\n",
    "```python\n",
    "# Parametric: main signal\n",
    "mu_theory = LCDM_model(z, H0, Omega_m)\n",
    "\n",
    "# Data-driven: capture deviations\n",
    "residuals = mu_obs - mu_theory\n",
    "gp = GaussianProcessRegressor()\n",
    "gp.fit(z, residuals)\n",
    "\n",
    "# Final prediction\n",
    "mu_final = mu_theory + gp.predict(z_new)\n",
    "```\n",
    "\n",
    "**Use case:** ΛCDM is mostly right, but there are small deviations (maybe new physics!)\n",
    "\n",
    "### Example 2: Physics-Informed Neural Networks\n",
    "```python\n",
    "# Neural network (data-driven) \n",
    "# but loss function includes physics equations (parametric constraints)\n",
    "loss = data_loss + lambda * physics_loss\n",
    "```\n",
    "\n",
    "**Use case:** Complex system where exact solution unknown, but physics provides constraints\n",
    "\n",
    "---\n",
    "\n",
    "## Philosophy: Two Ways of Knowing\n",
    "\n",
    "### Parametric (Deductive)\n",
    "**\"Top-down\" reasoning:**\n",
    "1. Start with theory (General Relativity)\n",
    "2. Derive predictions (μ(z) formula)\n",
    "3. Fit parameters to data (H₀, Ωₘ)\n",
    "4. Confirm or reject theory\n",
    "\n",
    "**Example reasoning:**\n",
    "> \"Einstein's equations predict this relationship. If the universe contains matter and dark energy in these proportions, we should see this distance-redshift curve.\"\n",
    "\n",
    "### Data-Driven (Inductive)\n",
    "**\"Bottom-up\" reasoning:**\n",
    "1. Start with data\n",
    "2. Find patterns\n",
    "3. Build empirical model\n",
    "4. (Maybe) develop theory later\n",
    "\n",
    "**Example reasoning:**\n",
    "> \"The data shows this smooth curve. I don't know why, but I can predict intermediate values accurately.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Tips\n",
    "\n",
    "### For Your Homework/Analysis:\n",
    "\n",
    "**First:** Do data-driven (GP) analysis\n",
    "- ✅ Explore the data without bias\n",
    "- ✅ Check for anomalies or unexpected features\n",
    "- ✅ Establish that data is smooth and well-behaved\n",
    "\n",
    "**Then:** Do parametric (ΛCDM) fit\n",
    "- ✅ Test physical model\n",
    "- ✅ Extract meaningful parameters (H₀, Ωₘ)\n",
    "- ✅ Compare to theory predictions\n",
    "\n",
    "**Finally:** Compare approaches\n",
    "- Do residuals (data - ΛCDM) show patterns?\n",
    "- Does GP capture something ΛCDM misses?\n",
    "- Is ΛCDM adequate or do we need new physics?\n",
    "\n",
    "---\n",
    "\n",
    "## Common Misconceptions\n",
    "\n",
    "### ❌ \"Parametric means linear regression\"\n",
    "**Reality:** Parametric just means fixed functional form. Can be highly nonlinear (like ΛCDM integral!)\n",
    "\n",
    "### ❌ \"Data-driven is always better because it's more flexible\"\n",
    "**Reality:** Flexibility can lead to overfitting. Without theory, you might fit noise.\n",
    "\n",
    "### ❌ \"GP has no parameters\"\n",
    "**Reality:** GP has hyperparameters (kernel, length scales). They're just not the \"parameters of interest\" like H₀.\n",
    "\n",
    "### ❌ \"Parametric models can't be wrong\"\n",
    "**Reality:** Theory-based models can absolutely be wrong! That's why we test them.\n",
    "\n",
    "### ❌ \"You must choose one approach\"\n",
    "**Reality:** Use both! They provide complementary information.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Analogy\n",
    "\n",
    "### Parametric: Following a Recipe\n",
    "- Recipe says: \"Bake at 180°C for 30 minutes\"\n",
    "- Based on theory: chemistry of baking\n",
    "- Parameters: temperature, time\n",
    "- If you follow recipe at 200°C for 60 min, you can predict (burnt cake)\n",
    "\n",
    "### Data-Driven: Tasting and Adjusting\n",
    "- Try different amounts, taste, adjust\n",
    "- No recipe, just experience\n",
    "- Learn empirically: \"More sugar → sweeter\"\n",
    "- Can't predict what happens with caviar (outside training data)\n",
    "\n",
    "**Best approach:** Use recipe (theory) as starting point, but adjust based on tasting (data)!\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Feature | Parametric | Data-Driven |\n",
    "|---------|-----------|-------------|\n",
    "| **Basis** | Physical theory | Data patterns |\n",
    "| **# Parameters** | Few (2-10) | Many (or infinite) |\n",
    "| **Interpretability** | High | Low |\n",
    "| **Flexibility** | Low | High |\n",
    "| **Extrapolation** | Good | Poor |\n",
    "| **Overfitting risk** | Low | High |\n",
    "| **Requires theory** | Yes | No |\n",
    "| **Best for** | Well-understood physics | Exploratory analysis |\n",
    "| **Your SN example** | ΛCDM (H₀, Ωₘ) | Gaussian Process |\n",
    "\n",
    "---\n",
    "\n",
    "## The Bottom Line\n",
    "\n",
    "**Parametric:** \n",
    "> \"I know the physics, I just need to measure the parameters.\"\n",
    "\n",
    "**Data-Driven:** \n",
    "> \"I don't know the physics, let the data speak.\"\n",
    "\n",
    "**Best Practice:**\n",
    "> \"Use data-driven to explore, parametric to explain.\"\n",
    "\n",
    "For cosmology: Start with GP to ensure no surprises, then fit ΛCDM to extract physical parameters!\n",
    "\n",
    "🎯 **Remember:** Neither is \"better\" - they answer different questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b60f3-bcc9-4c8f-991b-7d5d6e8aaa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa41092-6f3f-488e-ab19-9fa445d8d7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0478f4-e1c3-4570-8bac-876ae04b2f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
